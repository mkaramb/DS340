{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"author":[{"@type":"Person","name":"Kevin Gold"}]},"cells":[{"cell_type":"markdown","metadata":{"id":"e4yvI-kf-myN"},"source":["# MDPs and Q-learning On \"Ice\" (60 points possible)"]},{"cell_type":"markdown","metadata":{"id":"-iEuDHFs-ixs"},"source":["In this assignment, we’ll revisit Markov Decision Processes while also trying out Q-Learning, the reinforcement learning approach that associates utilities with attempting actions in states.\n","The problem that we’re attempting to solve is the following:\n","\n","1.  There is a grid of spaces in a rectangle.  Each space can contain a pit (negative reward), gold (positive reward), or nothing.\n","2.  The rectangle is effectively surrounded by walls, so anything that would move you outside the rectangle, instead moves you to the edge of the rectangle.\n","3.  The floor is icy.  Any attempt to move in a cardinal direction results in moving a somewhat random number of spaces in that direction.  The exact probabilities of moving each number of spaces are given in the problem description.  (If you slide too far, see rule #2.)\n","4.  Landing on a pit or gold effectively “ends the run,” for both a Q learner and an agent later trying out the policy.  It’s game over.  (To simulate this during Q learning, set all Q values for the space to equal its reward, then start over from a random space.)  Note that it's still possible to slide past a pit or gold - this doesn't end the run.\n","\n","A sample input looks like this:\n"]},{"cell_type":"code","metadata":{"id":"L_PSiQiO-_2y","executionInfo":{"status":"ok","timestamp":1668736949586,"user_tz":300,"elapsed":3,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"source":["sampleMDP = \"\"\"0.7 0.2 0.1\n","- - P - -\n","- - G P -\n","- - P - -\n","- - - - -\"\"\""],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3CmLASMB_Gsd"},"source":["\n","The first line says that the probabilities of moving one, two, or three spaces in the direction of movement are 0.7, 0.2, and 0.1.   The rest is a map of the environment, where a dash is an empty space, P is a pit, and G is gold.\n","\n","Your job is to finish the code below for mdp_solve() and q_solve().  These take a problem description like the one pictured above, and return a policy giving the recommended action to take in each empty square (U=up, R=right, D=down, L=left).\n","\n","**1, 19 points)**  mdp_solve() should use value iteration and the Bellman equation.  ITERATIONS will refer to the number of complete passes you perform over all states.  You can initialize the utilities to the rewards of each state.  Don’t update the rewards spaces from their initial rewards; since they end the trial, they have no future utility.  Don't update utilities in-place as you iterate through them, but create a fresh array of utilities with each pass, in order to avoid biasing moves in the directions that have already been updated.\n","\n","**2, 26 points)**  q_solve() will run ITERATIONS trials in which a learner starts in a random empty square and moves until it hits a pit or gold, in which case, the trial is over.  (If it was randomly dropped into gold or a pit, the trial is immediately over.)  The learner moves by deciding randomly whether to choose a random direction (with probability EXPLORE_PROB) or move according to the best Q-value of its current square (otherwise).  Simulate the results of the move on slippery ice to determine where the learner ended up - then apply the Q-learning equation given in lecture and the textbook.  (There are multiple Q-learning variants out there, so try to use the equations and practices described in lecture instead of using other sources, to avoid confusion.)\n","\n","The fact that a trial ends immediately on finding gold or a pit means that we want to handle those spaces in a special way.  Normally Q values are updated on moving to the next state, but we won’t see any next state in these cases.  So, to handle this, when the agent discovers one of these rewards, set all the Q values for that space to the associated reward before quitting the trial.  So, for example, if gold is worth 100 and it’s discovered in square x, Q(x,UP) = 100, Q(x,RIGHT) = 100, Q(x, DOWN) = 100, and Q(x, LEFT) = 100.  There’s no need to apply the rest of the Q update equation when the trial is ending, because that’s all about future rewards, and there’s no future when the trial is ending.  But now the spaces that can reach that space will evaluate themselves appropriately.  (Before being \"discovered,\" the square should have no utility.)\n","\n","You should use the GOLD_REWARD, PIT_REWARD, LEARNING_RATE, and DISCOUNT_FACTOR constants at the top of the code box below.\n","\n","Q-learning involves a lot of randomness and some arbitrary decisions when breaking ties, so two implementations can both be correct but recommend slightly different policies in the end, even if they have the same starting random seed.  While we provide some helpful premade maps below, your main guide for debugging will be common sense in deciding whether the policy created by your agent makes sense -- ie, agents following the policy will get gold without taking unnecessary risks."]},{"cell_type":"code","metadata":{"id":"ZW7aHFXpUQ9l","executionInfo":{"status":"ok","timestamp":1668737936625,"user_tz":300,"elapsed":192,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"source":["\"\"\" \"MDPs on Ice - Assignment 5\"\"\"\n","\n","import random\n","import copy\n","import numpy as np\n","\n","GOLD_REWARD = 250.0\n","PIT_REWARD = -150.0\n","DISCOUNT_FACTOR = 0.8\n","EXPLORE_PROB = 0.2 # for Q-learning\n","LEARNING_RATE = 0.01\n","ITERATIONS = 100000\n","MAX_MOVES = 1000\n","ACTIONS = 4\n","UP = 0\n","RIGHT = 1\n","DOWN = 2\n","LEFT = 3\n","MOVES = ['U', 'R', 'D', 'L']\n","\n","# Fixed random number generator seed for result reproducibility --\n","# don't use a random number generator besides this to match sol\n","random.seed(340)\n","\n","class Problem:\n","    \"\"\"Represents the physical space, transition probabilities, reward locations, and approach\n","\n","    ...in short, the info in the problem string\n","\n","    Attributes:\n","        move_probs (List[float]):  probabilities of going 1,2,3 spaces\n","        map (List[List(string)]]:  \"-\" (safe, empty space), \"G\" (gold), \"P\" (pit)\n","\n","    String format consumed looks like\n","    0.7 0.2 0.1   [probability of going 1, 2, 3 spaces]\n","    - - - - - - P - - - -   [space-delimited map rows]\n","    - - G - - - - - P - -   [G is gold, P is pit]\n","\n","    You can assume the maps are rectangular, although this isn't enforced\n","    by this constructor.\n","    \"\"\"\n","\n","    def __init__(self, probstring):\n","        \"\"\" Consume string formatted as above\"\"\"\n","        self.map = []\n","        for i, line in enumerate(probstring.splitlines()):\n","            if i == 0:\n","                self.move_probs = [float(s) for s in line.split()]\n","            else:\n","                self.map.append(line.split())\n","\n","    def solve(self, iterations, use_q):\n","        \"\"\" Wrapper for MDP and Q solvers.\n","\n","        Args:\n","            iterations (int):  Number of iterations (but these work differently for the two solvers)\n","            use_q (bool):  False means use MDP value iteration, true means use Q-learning\n","        Returns:\n","            A Policy, in either case (what to do in each square; see class below)\n","        \"\"\"\n","\n","        if use_q:\n","            return q_solve(self, iterations)\n","        return mdp_solve(self, iterations)\n","\n","class Policy:\n","    \"\"\" Abstraction on the best action to perform in each state.\n","\n","    This is a string list-of-lists map similar to the problem input, but a character gives the best\n","    action to take in each non-reward square (see MOVES constant at top of file).\n","    \"\"\"\n","\n","    def __init__(self, problem):\n","        \"\"\"Args:\n","\n","        problem (Problem):  The MDP problem this is a policy for\n","        \"\"\"\n","        self.best_actions = copy.deepcopy(problem.map)\n","\n","    def __str__(self):\n","        \"\"\"Join the characters in the policy into one big space-separated, multline string\"\"\"\n","        return '\\n{}\\n'.format('\\n'.join([' '.join(row) for row in self.best_actions]))\n","\n","def roll_steps(move_probs, row, col, move, rows, cols):\n","    \"\"\"Calculates the new coordinates that result from a move.\n","\n","    Includes the \"roll of the dice\" for transition probabilities and checking arena boundaries.\n","\n","    Helper for try_policy and q_solve - probably useful in your Q-learning implementation.\n","\n","    Args:\n","        move_probs (List[float]):  Transition probabilities for the ice (from problem)\n","        row, col (int, int):  location of agent before moving\n","        move (string):  The direction of move as a MOVES character (not an int constant!)\n","        rows, cols (int, int):  number of rows and columns in the map\n","\n","    Returns:\n","        new_row, new_col (int, int):  The new row and column after moving\n","    \"\"\"\n","    displacement = 1\n","    total_prob = 0\n","    move_sample = random.random()\n","    for p, prob in enumerate(move_probs):\n","        total_prob += prob\n","        if move_sample <= total_prob:\n","            displacement = p+1\n","            break\n","    # Handle \"slipping\" into edge of map\n","    new_row = row\n","    new_col = col\n","    if not isinstance(move, str):\n","        print(\"Warning: roll_steps wants str for move, got a different type\")\n","    if move == \"U\":\n","        new_row -= displacement\n","        if new_row < 0:\n","            new_row = 0\n","    elif move == \"R\":\n","        new_col += displacement\n","        if new_col >= cols:\n","            new_col = cols-1\n","    elif move == \"D\":\n","        new_row += displacement\n","        if new_row >= rows:\n","            new_row = rows-1\n","    elif move == \"L\":\n","        new_col -= displacement\n","        if new_col < 0:\n","            new_col = 0\n","    return new_row, new_col\n","\n","\n","def try_policy(policy, problem, iterations):\n","    \"\"\"Returns average utility per move of the policy.\n","\n","    Average utility is as measured by \"iterations\" random drops of an agent onto empty\n","    spaces, running until gold, pit, or time limit MAX_MOVES is reached.\n","\n","    Doesn't necessarily play a role in your code, but you can try policies this\n","    way\n","\n","    Args:\n","        policy (Policy):  the policy the agent is following\n","        problem (Problem):  the environment description\n","        iterations (int):  the number of random trials to run\n","    \"\"\"\n","    total_utility = 0\n","    total_moves = 0\n","    for _ in range(iterations):\n","        # Resample until we have an empty starting square\n","        while True:\n","            row = random.randrange(0, len(problem.map))\n","            col = random.randrange(0, len(problem.map[0]))\n","            if problem.map[row][col] == \"-\":\n","                break\n","        for moves in range(MAX_MOVES):\n","            total_moves += 1\n","            policy_rec = policy.best_actions[row][col]\n","            # Take the move - roll to see how far we go, bump into map edges as necessary\n","            row, col = roll_steps(problem.move_probs, row, col, policy_rec, \\\n","                                  len(problem.map), len(problem.map[0]))\n","            if problem.map[row][col] == \"G\":\n","                total_utility += GOLD_REWARD\n","                break\n","            if problem.map[row][col] == \"P\":\n","                total_utility += PIT_REWARD\n","                break\n","    return total_utility / total_moves\n","\n","def mdp_solve(problem, iterations):\n","    \"\"\" Perform value iteration for the given number of iterations on the MDP problem.\n","\n","    Here, the squares with rewards can be initialized to the reward values, since value iteration\n","    assumes complete knowledge of the environment and its rewards.\n","\n","    Args:\n","        problem (Problem):  description of the environment\n","        iterations (int):  number of complete passes over the utilities\n","    Returns:\n","        a Policy (though you may design this to return utilities as a second return value)\n","    \"\"\"\n","    # TODO calculate the policy\n","\n","    # initialize blank map\n","    bellman_map = [[0]*len(problem.map[0]) for x in range(len(problem.map))]\n","    \n","    # initialize rewards map\n","    for row in range(len(problem.map)):\n","        index = -1\n","        for square in range(len(problem.map[row])):\n","            index += 1\n","\n","            if problem.map[row][square] == 'P':\n","                bellman_map[row][index] = -150\n","\n","            elif problem.map[row][square] == 'G':\n","                bellman_map[row][index] = 250\n","\n","    # gloabl iteration loop\n","    for x in range(iterations):\n","\n","      # completes appropriate bellman calculation\n","      def bellman(move, prev_util, distance):\n","\n","          try:\n","              move_prob = problem.move_probs[distance]\n","          except IndexError:\n","              move_prob = 0\n","\n","          if move == 'P':\n","              util = move_prob * (PIT_REWARD + DISCOUNT_FACTOR * prev_util)\n","          if move == 'G':\n","              util = move_prob * (GOLD_REWARD + DISCOUNT_FACTOR * prev_util)\n","          else:\n","              util = move_prob * (0 + DISCOUNT_FACTOR * prev_util)\n","        \n","          return util\n","\n","      # neatly prints list of lists\n","      def prettyprint(map):\n","          for row in range(len(problem.map)):\n","              print('')\n","              for col in range(len(problem.map[0])):\n","                  print(str(map[row][col]) + (' ' * (6 - len(str(map[row][col])))), end='')\n","\n","      # row index tracker\n","      rowNum = -1\n","      for row in problem.map:\n","          rowNum += 1\n","\n","          for square in range(len(row)):\n","\n","              if problem.map[rowNum][square] == \"G\":\n","                  bellman_map[rowNum][square] = 250\n","                  continue\n","              elif problem.map[rowNum][square] == \"P\":\n","                  bellman_map[rowNum][square] = -150\n","                  continue\n","\n","              prev_util = bellman_map[rowNum][square]\n","\n","              for move in range(ACTIONS):\n","\n","                  if move == UP:\n","                      # upwards bellman logic\n","                      moveOutcomes = []\n","                      for i in range(rowNum-1, -1, -1):\n","                          moveOutcomes.append(problem.map[i][square])\n","\n","                      if len(moveOutcomes) == 0:\n","                          upUtil = 0\n","                      else:\n","                          upUtil = 0\n","                          for i in range(len(moveOutcomes[:3])):\n","                              upUtil += bellman(moveOutcomes[i], prev_util, i)\n","\n","                  elif move == RIGHT:\n","                      # right bellman logic\n","                      moveOutcomes = problem.map[rowNum][square+1:]\n","                      if len(moveOutcomes) == 0:\n","                          rightUtil = 0\n","                      else:\n","                          rightUtil = 0\n","                          for i in range(len(moveOutcomes[:3])):\n","                              rightUtil += bellman(moveOutcomes[i], prev_util, i)\n","\n","                  elif move == DOWN:\n","                      # down bellman logic\n","                      moveOutcomes = []\n","                      for i in range(3-rowNum):\n","                          moveOutcomes.append(problem.map[rowNum+i+1][square])\n","\n","                      if len(moveOutcomes) == 0:\n","                          downUtil = 0\n","                      else:\n","                          downUtil = 0\n","                          for i in range(len(moveOutcomes[:3])):\n","                              downUtil += bellman(moveOutcomes[i], prev_util, i)\n","\n","                  elif move == LEFT:\n","                      # left bellman logic\n","                      if (square) < 0:\n","                          leftUtil = 0\n","                      else:\n","                          moveOutcomes = problem.map[rowNum][:square]\n","\n","                          if len(moveOutcomes) == 0:\n","                              leftUtil = 0\n","                          else:\n","                              leftUtil = 0\n","                              if len(moveOutcomes) > 3:\n","                                  moveOutcomes = moveOutcomes[-3:]\n","\n","                              for i in range(len(moveOutcomes)-1, -1, -1):\n","                                  leftUtil += bellman(moveOutcomes[i], prev_util, 2-i)\n","\n","              # catches overflow bug for big_test\n","              try:\n","                  bellman_map[rowNum][square] = int(max(upUtil, rightUtil, downUtil, leftUtil))\n","              except OverflowError:\n","                  bellman_map[rowNum][square] = 10000\n","                  \n","    # debug code; prints input and output in neat format\n","\n","    if (x == iterations-1):\n","        for row in problem.map:\n","          print('')\n","          for square in row:\n","            print(square + ' ', end='')\n","        \n","        print('\\n')\n","\n","        prettyprint(bellman_map)\n","\n","        print('\\n\\n')\n","\n","    return bellman_map\n","\n","def q_solve(problem, iterations):\n","    \"\"\"q_solve:  Use Q-learning to find a good policy on an MDP problem.\n","\n","    Each iteration corresponds to a random drop of the agent onto the map, followed by moving\n","    the agent until a reward is reached or MAX_MOVES moves have been made.  When an agent\n","    is sitting on a reward, update the utility of each move from the space to the reward value\n","    and end the iteration.  (For simplicity, the agent also does this if just dropped there.)\n","    The agent does not \"know\" reward locations in its Q-values before encountering the space\n","    and \"discovering\" the reward.\n","\n","    Note that texts differ on when to pay attention to this reward - this code follows the\n","    convention of scoring rewards of the space you are moving *from*, plus discounted best q-value\n","    of where you landed.\n","\n","    Assume epsilon-greedy exploration.  Leave reward letters as-is in the policy,\n","    to make it more readable.\n","\n","    Args:\n","        problem (Problem):  The environment\n","        iterations (int):  The number of runs from random start to reward encounter\n","    Returns:\n","        A Policy for the map\n","    \"\"\"\n","    # TODO\n","\n","    def initializeRewardsTable(problem):\n","      mdpIteration = []\n","      for row in range(len(problem.map)):\n","          temp = []\n","          for square in problem.map[row]:\n","              if square == 'P':\n","                  temp.append(-150)\n","              elif square == 'G':\n","                  temp.append(250)\n","              else:\n","                  temp.append(0)\n","          mdpIteration.append(temp)\n","      return mdpIteration\n","\n","\n","    policy = Policy(problem)\n","    rewards = initializeRewardsTable(problem)\n","    rows = len(rewards)\n","    cols = len(rewards[0])\n","    actions = 4\n","    Qtable = [[[0,0,0,0] for c in range(cols)] for r in range(rows)]\n","    updateQtable = copy.deepcopy(Qtable)\n","    \n","    for i in range(iterations):\n","      x = random.randrange(0, len(problem.map))\n","      y = random.randrange(0, len(problem.map[0]))\n","      count = 0\n","\n","      while count <= MAX_MOVES:\n","        if rewards[x][y] == GOLD_REWARD or rewards[x][y] == PIT_REWARD:\n","          updateQtable[x][y] = [rewards[x][y]] * actions \n","          break\n","        else:\n","          if random.uniform(0, 1) <= EXPLORE_PROB:\n","            movenum = random.randrange(0,4)\n","            new_x, new_y = roll_steps(problem.move_probs, x, y, MOVES[movenum], rows, cols)\n","            newQ = new_q(rewards, Qtable, x, y, new_x, new_y, movenum)\n","            updateQtable[x][y][movenum] = newQ\n","            x = new_x\n","            y = new_y\n","          else:\n","            maxQ = max(Qtable[x][y])\n","            movenum = Qtable[x][y].index(maxQ)\n","            new_x, new_y = roll_steps(problem.move_probs, x, y, MOVES[movenum], rows, cols)\n","            newQ = new_q(rewards, Qtable, x, y, new_x, new_y, movenum)\n","            updateQtable[x][y][movenum] = newQ\n","            x = new_x\n","            y = new_y\n","          count += 1\n","          Qtable = copy.deepcopy(updateQtable)\n","\n","    for r in range(rows):\n","      for c in range(cols):\n","        if rewards[r][c] == 0:\n","          policy.best_actions[r][c] = str(Qtable[r][c].index(max(Qtable[r][c])))\n","\n","    return policy\n","\n","def new_q(rewards, utilities, r, c, new_r, new_c, movenum):\n","    \"\"\" Q-learning function.  Returns the new Q-value for space (r,c).\n","    It's recommended you code and test this before doing the overall Q-learning.\n","\n","    Should use the LEARNING_RATE and DISCOUNT_FACTOR.\n","\n","    Args:\n","        rewards (List[List[float]]):  Reward amounts built into the problem map (indexed [r][c])\n","        utilities (List[List[List[float]]]):  The Q-values for each action from each space.\n","                                              (Indexed as [row][col][move])\n","        r, c (int, int):  Row and column of our location before move\n","        new_r, new_c (int, int):  Row and column of our location after move\n","        movenum (int):  Integer index into the Q-values, corresponding to constants UP etc\n","    Returns:\n","        float - the new Q-value for the space we moved from\n","    \"\"\"\n","    # TODO\n","    new_q = rewards[new_r][new_c] + LEARNING_RATE * ((rewards[r][c] + DISCOUNT_FACTOR * utilities[r][c][movenum]) - max(utilities[new_r][new_c]))\n","    return new_q"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"5cuaEcLvAoYK","executionInfo":{"status":"ok","timestamp":1668736965519,"user_tz":300,"elapsed":3,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"source":["deterministic_test = \"\"\"1.0\n","- - P - -\n","- - G P -\n","- - P - -\n","- - - - -\"\"\""],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ABkxRiTA4Wi","executionInfo":{"status":"ok","timestamp":1668736967198,"user_tz":300,"elapsed":167,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"source":["# Notice that we counterintuitively are most likely to go 2 spaces here\n","very_slippy_test = \"\"\"0.2 0.7 0.1\n","- - P - -\n","- - G P -\n","- - P - -\n","- - - - -\"\"\""],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lqg8ZZUCBYYl","executionInfo":{"status":"ok","timestamp":1668736968355,"user_tz":300,"elapsed":2,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"source":["big_test = \"\"\"0.6 0.3 0.1\n","- P - G - P - - G -\n","P G - P - - - P - -\n","P P - P P - P - P -\n","P - - P P - - - - P\n","- - - - - - - - P G\"\"\""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"sHZ99I9uBmiH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668738034957,"user_tz":300,"elapsed":14757,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"e676e195-2946-441b-eac0-82003a1c4ff7"},"source":["# MDP value iteration tests\n","print(Problem(deterministic_test).solve(ITERATIONS, False))"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","- - P - - \n","- - G P - \n","- - P - - \n","- - - - - \n","\n","\n","0     0     -150  0     0     \n","0     1246  250   -150  0     \n","0     0     -150  0     0     \n","0     0     0     0     0     \n","\n","\n","[[0, 0, -150, 0, 0], [0, 1246, 250, -150, 0], [0, 0, -150, 0, 0], [0, 0, 0, 0, 0]]\n"]}]},{"cell_type":"code","metadata":{"id":"txLGS4pUwhh7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668738019111,"user_tz":300,"elapsed":13288,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"851c36cc-2fe5-469a-cb03-dcce6cc50fda"},"source":["print(Problem(sampleMDP).solve(ITERATIONS, False))"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","- - P - - \n","- - G P - \n","- - P - - \n","- - - - - \n","\n","\n","0     0     -150  0     0     \n","245   871   250   -150  245   \n","0     0     -150  0     0     \n","0     0     245   0     0     \n","\n","\n","[[0, 0, -150, 0, 0], [245, 871, 250, -150, 245], [0, 0, -150, 0, 0], [0, 0, 245, 0, 0]]\n"]}]},{"cell_type":"code","metadata":{"id":"WnprAX2uwiDI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668737997790,"user_tz":300,"elapsed":12976,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"b8302228-7418-483b-d1c8-2e5b362dc3b5"},"source":["print(Problem(very_slippy_test).solve(ITERATIONS, False))"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","- - P - - \n","- - G P - \n","- - P - - \n","- - - - - \n","\n","\n","0     0     -150  0     0     \n","871   245   250   -150  871   \n","0     0     -150  0     0     \n","0     0     871   0     0     \n","\n","\n","[[0, 0, -150, 0, 0], [871, 245, 250, -150, 871], [0, 0, -150, 0, 0], [0, 0, 871, 0, 0]]\n"]}]},{"cell_type":"code","metadata":{"id":"INhKxA6twic8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668737972739,"user_tz":300,"elapsed":28687,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"55337f17-b6aa-45de-de13-08ef2f1a1659"},"source":["print(Problem(big_test).solve(ITERATIONS, False))"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","- P - G - P - - G - \n","P G - P - - - P - - \n","P P - P P - P - P - \n","P - - P P - - - - P \n","- - - - - - - - P G \n","\n","\n","121   -150  746   250   746   -150  371   533   250   746   \n","-150  250   109   -150  121   0     0     -150  287   0     \n","-150  -150  0     -150  -150  0     -150  0     -150  0     \n","-150  371   0     -150  -150  0     0     0     121   -150  \n","0     121   0     0     0     0     121   265   -150  250   \n","\n","\n","[[121, -150, 746, 250, 746, -150, 371, 533, 250, 746], [-150, 250, 109, -150, 121, 0, 0, -150, 287, 0], [-150, -150, 0, -150, -150, 0, -150, 0, -150, 0], [-150, 371, 0, -150, -150, 0, 0, 0, 121, -150], [0, 121, 0, 0, 0, 0, 121, 265, -150, 250]]\n"]}]},{"cell_type":"code","metadata":{"id":"LfUJKMPtCRCs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668717685953,"user_tz":300,"elapsed":240906,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"bc3f9ecf-a457-4a91-b8cd-739d4c424ce6"},"source":["# Q-learning tests\n","# Set seed every time for consistent executions;\n","# comment out to get different random runs\n","random.seed(340)\n","print(Problem(deterministic_test).solve(ITERATIONS, True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0 0 P 0 0\n","0 1 G P 0\n","0 2 P 1 0\n","0 0 1 0 0\n","\n"]}]},{"cell_type":"code","metadata":{"id":"08cHCoI6wqak","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668718353896,"user_tz":300,"elapsed":436752,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"b710fcd3-7d77-4b2d-c2f2-79c304bfb875"},"source":["random.seed(340)\n","print(Problem(sampleMDP).solve(ITERATIONS, True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0 0 P 0 0\n","1 0 G P 0\n","2 2 P 1 1\n","2 2 1 1 0\n","\n"]}]},{"cell_type":"code","metadata":{"id":"PMM3kelxwqsx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668718695766,"user_tz":300,"elapsed":314607,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"6199022d-1ba9-4514-859d-490eb5ff74fe"},"source":["random.seed(340)\n","print(Problem(very_slippy_test).solve(ITERATIONS, True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","0 0 P 0 0\n","0 0 G P 0\n","2 2 P 1 1\n","2 0 1 1 1\n","\n"]}]},{"cell_type":"code","metadata":{"id":"hWu_w30AwrP9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668718964833,"user_tz":300,"elapsed":230336,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"59da1dc8-5fc4-40c9-e451-d23df684eaa0"},"source":["random.seed(340)\n","print(Problem(big_test).solve(ITERATIONS, True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","3 P 2 G 3 P 3 1 G 0\n","P G 2 P 0 2 3 P 0 1\n","P P 2 P P 2 P 2 P 1\n","P 2 2 P P 2 2 3 3 P\n","2 3 1 2 2 3 3 3 P G\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"RbP5VrvF9dIt"},"source":["Once you're done, here are a few thought questions (15 points total):\n","\n","**3, 3 points) In an earlier version of this code, students sometimes found that some bad luck with pits near the end of the Q-learning training could cause square (0,1) of sampleMDP to believe that going right was a bad idea.  What parameter would be best to adjust in order to mitigate the effect of bad luck near the end of a run?  In what direction should this parameter be adjusted?**"]},{"cell_type":"markdown","metadata":{"id":"MzD5X8wY-5gS"},"source":["**The best parameter to adjust would be to adjust use_q to TRUE, it should be applied to go in the down direction.**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-CkLwZnX-87e"},"source":["**4, 3 points) The value iteration MDP solver updates all squares an equal number of times.  The Q-learner does not.  Which squares might we expect the Q-learner to update the most?**"]},{"cell_type":"markdown","metadata":{"id":"AEhHogqO_7fC"},"source":["**We should expect the squares around the Golds and Pits to be updated the most compared to other squares.**"]},{"cell_type":"markdown","metadata":{"id":"DeGcHwKR_-m7"},"source":["**5, 9 points) Suppose we change the state information so that, instead of knowing its coordinates on the map, the agent instead knows just the locations of all rewards in a 5x5 square with the agent at the square center.  Thus, at the start of every run, it may not know exactly where it is, but it knows what is in the vicinity.  It also does not know the transition model.**\n","\n","**a, 3 points) We can't use value iteration here.  Why?**\n","\n","**b, 3 points) How many state-action combinations are possible, assuming the contents of the agent's own square don't matter?  Is a lookup table of Q-values feasible if we allocate memory for each possible state-action combination?  (Let's define \"feasible\" as \"able to be stored in a gig or less of memory,\" assuming 64-bit values.)**\n","\n","**c, 3 points) What is the most likely method that we would use to generalize in Q-learning from seen state-action combinations to situations that haven't been encountered before?**\n"]},{"cell_type":"markdown","metadata":{"id":"Inb8brIUIk8U"},"source":["**a) We can't use value iteration since we have a known rewards locations on the map so we cannot start a random value function since our values are based on the rewards locations.**\n","\n","**b) There would be 5^25 combination possibilities which would make the amount of tables needed to create this very challenging so your typical computer will msot likely run our of memory.**\n","\n","**c) We would use some type of non-Sarsa method to generalie a Q-learning function from seen state-action combinations to situations that haven't been encountered before.**"]},{"cell_type":"markdown","metadata":{"id":"78PjFoZBOLfp"},"source":["**Remember to submit your code on Blackboard as both an .ipynb (File->Download->.ipynb) and a PDF (Print->Save as PDF).**"]}]}