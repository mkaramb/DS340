{"cells":[{"cell_type":"markdown","id":"86f49ea4","metadata":{"id":"86f49ea4"},"source":["# Assignment 5 - 60 points possible\n","\n","## Apple, Bird, Cookie:\n","## Data from Google's Quick, Draw!  Game"]},{"cell_type":"markdown","id":"5c52a6d9","metadata":{"id":"5c52a6d9"},"source":["We will perform 2 tasks in this homework.  The first is to build a classifier that can tell whether a drawing is of an apple, a bird, or a cookie.  The data is taken from a game you can play online, developed by Google, called Quick, Draw!  The drawings are all Pictionary-style quick sketches of things.  Google has already done the work of turning the drawings into grayscale 28 x 28 images that are good for machine learning.  Since the images don't have that many pixels, the drawings are simplified, and there is a lot of data, this is an example of a relatively easy task for machine learning generally and neural networks in particular.  (There are many similar datasets here:  https://console.cloud.google.com/storage/browser/quickdraw_dataset/full/numpy_bitmap;tab=objects?pli=1&prefix=&forceOnObjectsSortingFiltering=false)\n","\n","Before you do anything else in this assignment, it's *strongly recommended* that you do two things with your Google Colab setup.  First, go to the upper right and select \"Connect to a hosted runtime.\"  Second, go to the menu at the top and select Runtime->Change runtime type->Pick GPU.  Selecting \"GPU\" will dramatically speed up the training times for the neural networks in this assignment.  But, picking GPU also resets the runtime, which is why you should do this before anything else.\n","\n","Download the two zip files associated with this homework at https://drive.google.com/file/d/14ZkNqKC34mUW5yUa6WjYWf1R-CempaoB/view?usp=sharing and https://drive.google.com/file/d/14UmGyFC_WSywNcm2yrCfe0x04IuA42O8/view?usp=sharing and place them in your own Google Drive.  Then run the code boxes below (possibly modifying the path) to mount the drive and unzip the files."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"MpMvOAecgMDQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667484100354,"user_tz":240,"elapsed":1822,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"4cc285d4-b3f6-4c39-938c-1ec272f557f3"},"id":"MpMvOAecgMDQ","execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# You may need to change \"NNAssign\" to your own directory name\n","!unzip gdrive/MyDrive/apple_bird_cookie.zip"],"metadata":{"id":"sMkp4tFNuoKx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667484108501,"user_tz":240,"elapsed":6466,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"babab272-0d4f-4f49-b8a7-372ea99eb42c"},"id":"sMkp4tFNuoKx","execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  gdrive/MyDrive/apple_bird_cookie.zip\n","  inflating: full_numpy_bitmap_apple.npy  \n","  inflating: __MACOSX/._full_numpy_bitmap_apple.npy  \n","  inflating: full_numpy_bitmap_bird.npy  \n","  inflating: __MACOSX/._full_numpy_bitmap_bird.npy  \n","  inflating: full_numpy_bitmap_cookie.npy  \n","  inflating: __MACOSX/._full_numpy_bitmap_cookie.npy  \n"]}]},{"cell_type":"code","execution_count":12,"id":"3df7fbe2","metadata":{"id":"3df7fbe2","executionInfo":{"status":"ok","timestamp":1667484111665,"user_tz":240,"elapsed":929,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"outputs":[],"source":["import numpy as np\n","\n","apples = np.load('full_numpy_bitmap_apple.npy')\n","birds = np.load('full_numpy_bitmap_bird.npy')\n","cookies = np.load('full_numpy_bitmap_cookie.npy')"]},{"cell_type":"markdown","id":"215adfc5","metadata":{"id":"215adfc5"},"source":["(1, 2pts) Call np.concatenate to join the three datasets together into a single array called \"all_kinds\".  Notice that the three datasets should be passed to np.concatenate as a tuple.\n"]},{"cell_type":"code","execution_count":13,"id":"c687d765","metadata":{"id":"c687d765","executionInfo":{"status":"ok","timestamp":1667484115138,"user_tz":240,"elapsed":323,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"outputs":[],"source":["# TODO concatenate the data\n","all_kinds = np.concatenate((apples, birds, cookies))"]},{"cell_type":"markdown","id":"a2bb4409","metadata":{"id":"a2bb4409"},"source":["(2, 3 pts) Now we need a list of labels that is as long as all_kinds.  Create a list named \"labels\" that is as long as all_kinds, where each element identifies which kind of drawing can be found at that place in all_kinds.  Label apples as 0, birds as 1, and cookies as 2.  (You may find it useful to call len() on the apples, birds, and cookies arrays.)"]},{"cell_type":"code","execution_count":14,"id":"9eaadef5","metadata":{"id":"9eaadef5","executionInfo":{"status":"ok","timestamp":1667484118601,"user_tz":240,"elapsed":364,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"outputs":[],"source":["# TODO make labels\n","labels = [0]*len(apples) + [1]*len(birds) + [2]*len(cookies)"]},{"cell_type":"markdown","id":"16c9a7f4","metadata":{"id":"16c9a7f4"},"source":["Now we'll call train_test_split to separate the data into training and testing data."]},{"cell_type":"code","execution_count":15,"id":"d1c6e971","metadata":{"id":"d1c6e971","executionInfo":{"status":"ok","timestamp":1667484121879,"user_tz":240,"elapsed":1021,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(all_kinds, labels)"]},{"cell_type":"markdown","id":"49f0afaa","metadata":{"id":"49f0afaa"},"source":["(3, 2pts) This data ranges from 0 to 255, but neural networks tend to work best when the data is between 0 and 1.\n","Scale the train and test data by dividing it by 255."]},{"cell_type":"code","execution_count":16,"id":"378d00d0","metadata":{"id":"378d00d0","executionInfo":{"status":"ok","timestamp":1667484126735,"user_tz":240,"elapsed":2865,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"outputs":[],"source":["# TODO scale the data\n","x_test = x_test/255\n","x_train = x_train/255"]},{"cell_type":"markdown","id":"dcf559ba","metadata":{"id":"dcf559ba"},"source":["(4, 5pts) If we examine the shape of x, it isn't quite what we want yet - it is an array of arrays that are of size length-of-data x 784.  784 is 28x28, and we want our convolutional neural networks to perceive the data as a 28x28 square instead of one long array of 784 elements.  Call x_train.reshape() and x_test.reshape() so that their dimensions are length-of-data x 28 x 28.  (Note that reshape expects a tuple that is the size as its argument.  You can pass -1 as one of the dimensions if you don't want to figure out how long the array is.)"]},{"cell_type":"code","execution_count":17,"id":"c6c77691","metadata":{"id":"c6c77691","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667484130378,"user_tz":240,"elapsed":393,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"fb6b6905-ec0c-4cf9-fd4a-a648596fab6c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(784,)"]},"metadata":{},"execution_count":17}],"source":["x_train[0].shape"]},{"cell_type":"code","execution_count":18,"id":"ec1618dd","metadata":{"id":"ec1618dd","executionInfo":{"status":"ok","timestamp":1667484133458,"user_tz":240,"elapsed":421,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"outputs":[],"source":["# TODO reshape x_train\n","x_train = x_train.reshape((-1,28,28))"]},{"cell_type":"code","execution_count":19,"id":"06703e7c","metadata":{"id":"06703e7c","executionInfo":{"status":"ok","timestamp":1667484138363,"user_tz":240,"elapsed":351,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"outputs":[],"source":["# TODO reshape x_test\n","x_test = x_test.reshape((-1,28,28))"]},{"cell_type":"markdown","id":"d9cad60c","metadata":{"id":"d9cad60c"},"source":["If your reshape worked correctly, the following code should show a apple, bird, or cookie sketch.  If it didn't work, it will look like random noise.  If you're not sure, you can always try more images."]},{"cell_type":"code","execution_count":21,"id":"b88980bd","metadata":{"id":"b88980bd","colab":{"base_uri":"https://localhost:8080/","height":283},"executionInfo":{"status":"ok","timestamp":1667484145742,"user_tz":240,"elapsed":415,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"a991670e-7189-41f2-f110-0b0abb28f5bd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f54c89c8ad0>"]},"metadata":{},"execution_count":21},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import matplotlib.pyplot as plt\n","plt.imshow(x_train[0]) # Should be a recognizable image if we reshaped correctly"]},{"cell_type":"markdown","id":"604fdb13","metadata":{"id":"604fdb13"},"source":["As the last step before creating the neural network, we need to turn the labels into one-hot encodings, like [0,0,1] instead of 2.  There's a handy keras function that does this, and we're just going to call it for you."]},{"cell_type":"code","execution_count":22,"id":"862986ee","metadata":{"id":"862986ee","executionInfo":{"status":"ok","timestamp":1667484161264,"user_tz":240,"elapsed":376,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"outputs":[],"source":["from tensorflow import keras\n","num_classes = 3\n","y_train = keras.utils.to_categorical(y_train, num_classes)\n","y_test = keras.utils.to_categorical(y_test, num_classes)"]},{"cell_type":"markdown","id":"c3d7790c","metadata":{"id":"c3d7790c"},"source":["(5, 9pts) Rather than create a network completely from scratch, it makes sense to start with a network that you know does something similar.  One of the keras tutorials shows how to do digit recognition with a relatively small deep neural network.  The dataset, the MNIST digit dataset, is very similar to ours because it consists of 28 x 28 black and white line drawings.  Consult the \"Build a Model\" and \"Train the model\" sections of https://keras.io/examples/vision/mnist_convnet/ and get that neural network running on our apple, bird, and cookie data.  (Be sure to leave a comment indicating that you borrowed the structure from there.)\n"]},{"cell_type":"code","execution_count":23,"id":"07638a47","metadata":{"id":"07638a47","executionInfo":{"status":"ok","timestamp":1667484163872,"user_tz":240,"elapsed":320,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}}},"outputs":[],"source":["# TODO borrow the MNIST model with attribution\n","from tensorflow.keras import layers\n","import keras\n","\n","input_shape = (28,28,1)\n","model = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation=\"softmax\"),\n","    ]\n",")\n"]},{"cell_type":"code","execution_count":24,"id":"044efa90","metadata":{"id":"044efa90","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667484346244,"user_tz":240,"elapsed":179872,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"7edb434f-9ff9-41ae-b504-4c3d1b06737a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","2161/2161 [==============================] - 13s 5ms/step - loss: 0.1321 - accuracy: 0.9588 - val_loss: 0.0825 - val_accuracy: 0.9742\n","Epoch 2/15\n","2161/2161 [==============================] - 12s 6ms/step - loss: 0.0856 - accuracy: 0.9730 - val_loss: 0.0714 - val_accuracy: 0.9760\n","Epoch 3/15\n","2161/2161 [==============================] - 12s 5ms/step - loss: 0.0750 - accuracy: 0.9760 - val_loss: 0.0647 - val_accuracy: 0.9792\n","Epoch 4/15\n","2161/2161 [==============================] - 11s 5ms/step - loss: 0.0694 - accuracy: 0.9775 - val_loss: 0.0594 - val_accuracy: 0.9802\n","Epoch 5/15\n","2161/2161 [==============================] - 12s 5ms/step - loss: 0.0654 - accuracy: 0.9787 - val_loss: 0.0571 - val_accuracy: 0.9810\n","Epoch 6/15\n","2161/2161 [==============================] - 14s 6ms/step - loss: 0.0626 - accuracy: 0.9795 - val_loss: 0.0543 - val_accuracy: 0.9817\n","Epoch 7/15\n","2161/2161 [==============================] - 12s 5ms/step - loss: 0.0599 - accuracy: 0.9803 - val_loss: 0.0531 - val_accuracy: 0.9826\n","Epoch 8/15\n","2161/2161 [==============================] - 12s 5ms/step - loss: 0.0586 - accuracy: 0.9806 - val_loss: 0.0534 - val_accuracy: 0.9819\n","Epoch 9/15\n","2161/2161 [==============================] - 11s 5ms/step - loss: 0.0577 - accuracy: 0.9810 - val_loss: 0.0544 - val_accuracy: 0.9820\n","Epoch 10/15\n","2161/2161 [==============================] - 12s 5ms/step - loss: 0.0558 - accuracy: 0.9813 - val_loss: 0.0513 - val_accuracy: 0.9826\n","Epoch 11/15\n","2161/2161 [==============================] - 12s 6ms/step - loss: 0.0549 - accuracy: 0.9818 - val_loss: 0.0498 - val_accuracy: 0.9830\n","Epoch 12/15\n","2161/2161 [==============================] - 11s 5ms/step - loss: 0.0543 - accuracy: 0.9817 - val_loss: 0.0500 - val_accuracy: 0.9827\n","Epoch 13/15\n","2161/2161 [==============================] - 12s 6ms/step - loss: 0.0534 - accuracy: 0.9820 - val_loss: 0.0504 - val_accuracy: 0.9832\n","Epoch 14/15\n","2161/2161 [==============================] - 11s 5ms/step - loss: 0.0528 - accuracy: 0.9821 - val_loss: 0.0505 - val_accuracy: 0.9834\n","Epoch 15/15\n","2161/2161 [==============================] - 11s 5ms/step - loss: 0.0518 - accuracy: 0.9825 - val_loss: 0.0493 - val_accuracy: 0.9839\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f54c8948950>"]},"metadata":{},"execution_count":24}],"source":["# TODO compile and fit the model\n","batch_size = 128\n","epochs = 15\n","\n","model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"]},{"cell_type":"markdown","id":"1de32e51","metadata":{"id":"1de32e51"},"source":["(6, 2pts) Evaluate the model on the test set with the code below.  You should have a test accuracy well above 90%."]},{"cell_type":"code","execution_count":25,"id":"57781026","metadata":{"id":"57781026","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667484359862,"user_tz":240,"elapsed":7740,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"e6ceaa91-8221-428b-e2a4-021ce4be0ea9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss: 0.04904060438275337\n","Test accuracy: 0.9836249947547913\n"]}],"source":["score = model.evaluate(x_test, y_test, verbose=0)\n","print(\"Test loss:\", score[0])\n","print(\"Test accuracy:\", score[1])"]},{"cell_type":"markdown","id":"ce02d1d3","metadata":{"id":"ce02d1d3"},"source":["## Cats and Dogs\n","\n","It was relatively easy to get good performance on that task, because the size of each input is small and the features needed for success weren't too complicated.  We'll now try a classification task with real images.  This is a moderately well-known \"cats and dogs\" dataset.  Unzip the dogs-vs-cats dataset in the current directory.  You can examine the files to see pictures of cats and dogs with varying dimensions and varying poses."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"metadata":{"id":"d_7Hbn3ug0YC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667483484849,"user_tz":240,"elapsed":26496,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"43c551bd-2437-4381-cff7-8658134674e9"},"id":"d_7Hbn3ug0YC","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["!unzip gdrive/MyDrive/dogs-vs-cats.zip"],"metadata":{"id":"OUdMf-mCg24l"},"id":"OUdMf-mCg24l","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!ls"],"metadata":{"id":"RUN0ls0qmat_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667483527371,"user_tz":240,"elapsed":324,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"f5c4f07e-538e-4bf5-f07c-0b0f79d334c7"},"id":"RUN0ls0qmat_","execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["dogs-vs-cats  gdrive  __MACOSX\tsample_data\n"]}]},{"cell_type":"code","execution_count":7,"id":"6928900a","metadata":{"id":"6928900a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667483534793,"user_tz":240,"elapsed":3586,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"8f6f7865-39c9-40c8-ae72-4c800a467af2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found 25000 files belonging to 2 classes.\n","Using 20000 files for training.\n","Found 25000 files belonging to 2 classes.\n","Using 5000 files for validation.\n"]}],"source":["import tensorflow as tf\n","image_size = (180, 180)\n","batch_size = 32\n","# Data from https://www.kaggle.com/competitions/dogs-vs-cats/\n","# Code from https://keras.io/examples/vision/image_classification_from_scratch/\n","train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    \"dogs-vs-cats/train\",\n","    validation_split=0.2,\n","    subset=\"training\",\n","    seed=1337,\n","    image_size=image_size,\n","    batch_size=batch_size,\n",")\n","val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n","    \"dogs-vs-cats/train\",\n","    validation_split=0.2,\n","    subset=\"validation\",\n","    seed=1337,\n","    image_size=image_size,\n","    batch_size=batch_size,\n",")"]},{"cell_type":"markdown","id":"f21331dd","metadata":{"id":"f21331dd"},"source":["(7, 9pts) We'll suppose that the closest starting point network we have on hand for this is the MNIST network again.  Adapt it to this dataset with the following changes:\n","\n","* The input shape is 180 x 180 x 3.\n","* Put the rescaling of dividing values by 255 in the network itself with a layers.Rescaling() layer after the Input layer.  https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling\n","* The final \"softmax\" activation function is only appropriate for multiclass classification.  Change this to a more appropriate activation function for binary classification.\n","* Name the model model2 to work with the training code below."]},{"cell_type":"code","execution_count":null,"id":"2f953ab4","metadata":{"id":"2f953ab4"},"outputs":[],"source":["# TODO create model2\n","from tensorflow.keras import layers\n","import keras\n","\n","input_shape = (180,180,3)\n","num_classes = 1\n","model2 = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        layers.Rescaling(1./255),\n","        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation=\"sigmoid\"),\n","    ]\n",")"]},{"cell_type":"markdown","id":"ef1ee68b","metadata":{"id":"ef1ee68b"},"source":["You can train the network using the following code."]},{"cell_type":"code","execution_count":null,"id":"d4075fc4","metadata":{"id":"d4075fc4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667412704708,"user_tz":240,"elapsed":570253,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"1ef1f41f-d2ac-461b-dd76-7f992991d3ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","625/625 [==============================] - 36s 56ms/step - loss: 0.6002 - accuracy: 0.6678 - val_loss: 0.5320 - val_accuracy: 0.7296\n","Epoch 2/15\n","625/625 [==============================] - 37s 59ms/step - loss: 0.5030 - accuracy: 0.7534 - val_loss: 0.4978 - val_accuracy: 0.7596\n","Epoch 3/15\n","625/625 [==============================] - 39s 63ms/step - loss: 0.4518 - accuracy: 0.7918 - val_loss: 0.4968 - val_accuracy: 0.7650\n","Epoch 4/15\n","625/625 [==============================] - 36s 58ms/step - loss: 0.4076 - accuracy: 0.8122 - val_loss: 0.4822 - val_accuracy: 0.7790\n","Epoch 5/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.3695 - accuracy: 0.8325 - val_loss: 0.4952 - val_accuracy: 0.7824\n","Epoch 6/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.3339 - accuracy: 0.8521 - val_loss: 0.4805 - val_accuracy: 0.7942\n","Epoch 7/15\n","625/625 [==============================] - 36s 57ms/step - loss: 0.3058 - accuracy: 0.8684 - val_loss: 0.5114 - val_accuracy: 0.7822\n","Epoch 8/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.2807 - accuracy: 0.8789 - val_loss: 0.5150 - val_accuracy: 0.7944\n","Epoch 9/15\n","625/625 [==============================] - 39s 62ms/step - loss: 0.2668 - accuracy: 0.8873 - val_loss: 0.5243 - val_accuracy: 0.7934\n","Epoch 10/15\n","625/625 [==============================] - 37s 58ms/step - loss: 0.2489 - accuracy: 0.8928 - val_loss: 0.5297 - val_accuracy: 0.7896\n","Epoch 11/15\n","625/625 [==============================] - 36s 57ms/step - loss: 0.2328 - accuracy: 0.9015 - val_loss: 0.5412 - val_accuracy: 0.7950\n","Epoch 12/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.2194 - accuracy: 0.9072 - val_loss: 0.5550 - val_accuracy: 0.7970\n","Epoch 13/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.2016 - accuracy: 0.9173 - val_loss: 0.5770 - val_accuracy: 0.7918\n","Epoch 14/15\n","625/625 [==============================] - 36s 57ms/step - loss: 0.1957 - accuracy: 0.9209 - val_loss: 0.5948 - val_accuracy: 0.7918\n","Epoch 15/15\n","625/625 [==============================] - 39s 62ms/step - loss: 0.1857 - accuracy: 0.9231 - val_loss: 0.6166 - val_accuracy: 0.7934\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6df4fa13d0>"]},"metadata":{},"execution_count":12}],"source":["epochs = 15\n","\n","model2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model2.fit(train_ds, epochs=epochs, validation_data=val_ds) # Validation data instead of fraction"]},{"cell_type":"markdown","id":"09a9c2b3","metadata":{"id":"09a9c2b3"},"source":["(8, 6pts) At this point, we should try to improve the network with a bit of architecture search.  Probably, the features extracted for the MNIST and apple/bird/cookie datasets are simpler than what's necessary for photos.  This implies needing deeper layers.  One strategy used by some well-known networks is to add blocks each consisting of a Conv2D layer followed by MaxPooling2D layer, where each Conv2D layer has twice as many filters as the last one (so 32, 64, 128 ...).  You can see our current architecture already follows this pattern.  Try training networks with one, two, and three more of these blocks between the last MaxPooling2D layer and the Flatten() call."]},{"cell_type":"code","execution_count":null,"id":"e26e5121","metadata":{"id":"e26e5121"},"outputs":[],"source":["# TODO define model3 with one more block of Conv2D and pooling\n","model3 = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        layers.Rescaling(1./255),\n","        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation=\"sigmoid\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"id":"ffd6ee5f","metadata":{"id":"ffd6ee5f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667413399218,"user_tz":240,"elapsed":588306,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"1e2b73e8-b449-43d9-8dfd-746c9b86933c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","625/625 [==============================] - 39s 61ms/step - loss: 0.5924 - accuracy: 0.6726 - val_loss: 0.5412 - val_accuracy: 0.7318\n","Epoch 2/15\n","625/625 [==============================] - 42s 67ms/step - loss: 0.4705 - accuracy: 0.7781 - val_loss: 0.4461 - val_accuracy: 0.7970\n","Epoch 3/15\n","625/625 [==============================] - 42s 67ms/step - loss: 0.4157 - accuracy: 0.8084 - val_loss: 0.4081 - val_accuracy: 0.8216\n","Epoch 4/15\n","625/625 [==============================] - 40s 64ms/step - loss: 0.3687 - accuracy: 0.8355 - val_loss: 0.3974 - val_accuracy: 0.8212\n","Epoch 5/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.3277 - accuracy: 0.8585 - val_loss: 0.3615 - val_accuracy: 0.8462\n","Epoch 6/15\n","625/625 [==============================] - 36s 58ms/step - loss: 0.2891 - accuracy: 0.8763 - val_loss: 0.3697 - val_accuracy: 0.8434\n","Epoch 7/15\n","625/625 [==============================] - 37s 59ms/step - loss: 0.2560 - accuracy: 0.8931 - val_loss: 0.3805 - val_accuracy: 0.8436\n","Epoch 8/15\n","625/625 [==============================] - 40s 63ms/step - loss: 0.2323 - accuracy: 0.9027 - val_loss: 0.3583 - val_accuracy: 0.8508\n","Epoch 9/15\n","625/625 [==============================] - 37s 58ms/step - loss: 0.2066 - accuracy: 0.9140 - val_loss: 0.3713 - val_accuracy: 0.8500\n","Epoch 10/15\n","625/625 [==============================] - 37s 58ms/step - loss: 0.1865 - accuracy: 0.9226 - val_loss: 0.3593 - val_accuracy: 0.8534\n","Epoch 11/15\n","625/625 [==============================] - 38s 61ms/step - loss: 0.1744 - accuracy: 0.9286 - val_loss: 0.3731 - val_accuracy: 0.8648\n","Epoch 12/15\n","625/625 [==============================] - 38s 61ms/step - loss: 0.1538 - accuracy: 0.9395 - val_loss: 0.3670 - val_accuracy: 0.8624\n","Epoch 13/15\n","625/625 [==============================] - 38s 61ms/step - loss: 0.1474 - accuracy: 0.9413 - val_loss: 0.4069 - val_accuracy: 0.8596\n","Epoch 14/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.1382 - accuracy: 0.9442 - val_loss: 0.4077 - val_accuracy: 0.8636\n","Epoch 15/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.1237 - accuracy: 0.9513 - val_loss: 0.4286 - val_accuracy: 0.8594\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6df4dd44d0>"]},"metadata":{},"execution_count":14}],"source":["epochs = 15\n","\n","model3.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model3.fit(train_ds, epochs=epochs, validation_data=val_ds)"]},{"cell_type":"code","source":["# TODO define model4 with two more blocks of Conv2D and pooling\n","model4 = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        layers.Rescaling(1./255),\n","        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation=\"sigmoid\"),\n","    ]\n",")"],"metadata":{"id":"TlJfMTjHs_J_"},"id":"TlJfMTjHs_J_","execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 15\n","\n","model4.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model4.fit(train_ds, epochs=epochs, validation_data=val_ds)"],"metadata":{"id":"J9zrSy54tC1g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667413990023,"user_tz":240,"elapsed":585816,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"462d2f6f-a580-43db-d27d-0562204960d2"},"id":"J9zrSy54tC1g","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","625/625 [==============================] - 39s 61ms/step - loss: 0.6257 - accuracy: 0.6334 - val_loss: 0.5219 - val_accuracy: 0.7358\n","Epoch 2/15\n","625/625 [==============================] - 37s 59ms/step - loss: 0.4955 - accuracy: 0.7577 - val_loss: 0.4560 - val_accuracy: 0.7954\n","Epoch 3/15\n","625/625 [==============================] - 38s 61ms/step - loss: 0.4093 - accuracy: 0.8130 - val_loss: 0.3553 - val_accuracy: 0.8460\n","Epoch 4/15\n","625/625 [==============================] - 39s 63ms/step - loss: 0.3437 - accuracy: 0.8499 - val_loss: 0.3300 - val_accuracy: 0.8566\n","Epoch 5/15\n","625/625 [==============================] - 36s 58ms/step - loss: 0.2878 - accuracy: 0.8784 - val_loss: 0.2959 - val_accuracy: 0.8736\n","Epoch 6/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.2456 - accuracy: 0.8968 - val_loss: 0.2561 - val_accuracy: 0.8896\n","Epoch 7/15\n","625/625 [==============================] - 43s 68ms/step - loss: 0.2061 - accuracy: 0.9147 - val_loss: 0.2426 - val_accuracy: 0.8958\n","Epoch 8/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.1790 - accuracy: 0.9251 - val_loss: 0.2389 - val_accuracy: 0.9004\n","Epoch 9/15\n","625/625 [==============================] - 38s 61ms/step - loss: 0.1530 - accuracy: 0.9367 - val_loss: 0.2393 - val_accuracy: 0.9026\n","Epoch 10/15\n","625/625 [==============================] - 39s 62ms/step - loss: 0.1302 - accuracy: 0.9464 - val_loss: 0.2650 - val_accuracy: 0.8926\n","Epoch 11/15\n","625/625 [==============================] - 38s 61ms/step - loss: 0.1165 - accuracy: 0.9531 - val_loss: 0.2493 - val_accuracy: 0.9090\n","Epoch 12/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.1017 - accuracy: 0.9599 - val_loss: 0.2597 - val_accuracy: 0.9048\n","Epoch 13/15\n","625/625 [==============================] - 36s 58ms/step - loss: 0.0901 - accuracy: 0.9647 - val_loss: 0.2739 - val_accuracy: 0.9080\n","Epoch 14/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.0905 - accuracy: 0.9645 - val_loss: 0.2716 - val_accuracy: 0.9058\n","Epoch 15/15\n","625/625 [==============================] - 39s 62ms/step - loss: 0.0723 - accuracy: 0.9729 - val_loss: 0.2918 - val_accuracy: 0.9062\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6df4c93710>"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# TODO model5 with three more blocks of Conv2D and pooling\n","model5 = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        layers.Rescaling(1./255),\n","        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(512, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dropout(0.5),\n","        layers.Dense(num_classes, activation=\"sigmoid\"),\n","    ]\n",")"],"metadata":{"id":"RVRi9HamtUdg"},"id":"RVRi9HamtUdg","execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 15\n","\n","model5.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model5.fit(train_ds, epochs=epochs, validation_data=val_ds)"],"metadata":{"id":"QanXeQYSteAW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667414601074,"user_tz":240,"elapsed":586303,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"419dbd0e-7372-48ca-d137-b407b6d2cfea"},"id":"QanXeQYSteAW","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/15\n","625/625 [==============================] - 39s 61ms/step - loss: 0.6359 - accuracy: 0.6259 - val_loss: 0.5609 - val_accuracy: 0.7336\n","Epoch 2/15\n","625/625 [==============================] - 38s 61ms/step - loss: 0.4993 - accuracy: 0.7549 - val_loss: 0.4504 - val_accuracy: 0.7806\n","Epoch 3/15\n","625/625 [==============================] - 37s 59ms/step - loss: 0.4090 - accuracy: 0.8110 - val_loss: 0.3616 - val_accuracy: 0.8334\n","Epoch 4/15\n","625/625 [==============================] - 40s 63ms/step - loss: 0.3318 - accuracy: 0.8540 - val_loss: 0.3059 - val_accuracy: 0.8656\n","Epoch 5/15\n","625/625 [==============================] - 39s 61ms/step - loss: 0.2672 - accuracy: 0.8865 - val_loss: 0.2669 - val_accuracy: 0.8854\n","Epoch 6/15\n","625/625 [==============================] - 39s 62ms/step - loss: 0.2165 - accuracy: 0.9079 - val_loss: 0.2528 - val_accuracy: 0.8910\n","Epoch 7/15\n","625/625 [==============================] - 38s 61ms/step - loss: 0.1773 - accuracy: 0.9269 - val_loss: 0.2612 - val_accuracy: 0.8966\n","Epoch 8/15\n","625/625 [==============================] - 38s 61ms/step - loss: 0.1468 - accuracy: 0.9402 - val_loss: 0.3067 - val_accuracy: 0.8804\n","Epoch 9/15\n","625/625 [==============================] - 39s 61ms/step - loss: 0.1186 - accuracy: 0.9510 - val_loss: 0.2960 - val_accuracy: 0.8948\n","Epoch 10/15\n","625/625 [==============================] - 41s 64ms/step - loss: 0.0970 - accuracy: 0.9629 - val_loss: 0.3103 - val_accuracy: 0.9036\n","Epoch 11/15\n","625/625 [==============================] - 38s 60ms/step - loss: 0.0877 - accuracy: 0.9676 - val_loss: 0.3167 - val_accuracy: 0.9002\n","Epoch 12/15\n","625/625 [==============================] - 39s 61ms/step - loss: 0.0782 - accuracy: 0.9705 - val_loss: 0.3463 - val_accuracy: 0.9022\n","Epoch 13/15\n","625/625 [==============================] - 39s 62ms/step - loss: 0.0731 - accuracy: 0.9714 - val_loss: 0.3019 - val_accuracy: 0.8982\n","Epoch 14/15\n","625/625 [==============================] - 37s 59ms/step - loss: 0.0577 - accuracy: 0.9782 - val_loss: 0.3049 - val_accuracy: 0.9046\n","Epoch 15/15\n","625/625 [==============================] - 40s 64ms/step - loss: 0.0664 - accuracy: 0.9753 - val_loss: 0.3504 - val_accuracy: 0.8988\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6df4aa3f10>"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["(9, 6pts) Architecture search could continue all day, but let's take a different approach to improving the network further.  When the accuracy on the training data is higher than the validation accuracy, this is evidence of overfitting.  Two approaches to handling overfitting are:\n","\n","* More dropout.  We have one big layer of dropout, but we could have one or more additional layers of Dropout(0.2).\n","* Data augmentation.  If the dataset is randomly rotated and flipped, this encourages features that are robust against these transformations, and it makes it harder to overfit the data.  We can work this into the pipeline itself with layers.RandomFlip(\"horizontal\") and layers.RandomRotation(0.1).\n","\n","Train a new network with these two ideas worked into your best architecture so far.  (Your best architecture is the one that you achieved the highest validation accuracy on.)  The dropout locations are up to you.  Use 20 epochs instead of 15, since these methods slow down the learning somewhat."],"metadata":{"id":"hztQQ92WCPlR"},"id":"hztQQ92WCPlR"},{"cell_type":"code","execution_count":null,"id":"326045fe","metadata":{"id":"326045fe"},"outputs":[],"source":["# TODO model6 with data augmentation and some extra dropout\n","model6 = keras.Sequential(\n","    [\n","        keras.Input(shape=input_shape),\n","        layers.Rescaling(1./255),\n","        layers.RandomFlip(\"horizontal\"),\n","        layers.RandomRotation(0.1),\n","        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(128, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Conv2D(256, kernel_size=(3, 3), activation=\"relu\"),\n","        layers.MaxPooling2D(pool_size=(2, 2)),\n","        layers.Flatten(),\n","        layers.Dropout(0.5),\n","        layers.Dropout(0.2),\n","        layers.Dense(num_classes, activation=\"sigmoid\"),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"id":"cfe4a031","metadata":{"id":"cfe4a031","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667416218864,"user_tz":240,"elapsed":1150220,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"361ee518-650d-4e9a-cfba-022a513c64cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","625/625 [==============================] - 55s 86ms/step - loss: 0.6457 - accuracy: 0.6187 - val_loss: 0.5671 - val_accuracy: 0.7080\n","Epoch 2/20\n","625/625 [==============================] - 52s 83ms/step - loss: 0.5583 - accuracy: 0.7170 - val_loss: 0.4781 - val_accuracy: 0.7730\n","Epoch 3/20\n","625/625 [==============================] - 54s 86ms/step - loss: 0.4939 - accuracy: 0.7648 - val_loss: 0.4424 - val_accuracy: 0.7974\n","Epoch 4/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.4531 - accuracy: 0.7893 - val_loss: 0.4035 - val_accuracy: 0.8104\n","Epoch 5/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.4199 - accuracy: 0.8052 - val_loss: 0.3694 - val_accuracy: 0.8368\n","Epoch 6/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.3910 - accuracy: 0.8260 - val_loss: 0.3440 - val_accuracy: 0.8496\n","Epoch 7/20\n","625/625 [==============================] - 54s 86ms/step - loss: 0.3620 - accuracy: 0.8391 - val_loss: 0.3350 - val_accuracy: 0.8536\n","Epoch 8/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.3453 - accuracy: 0.8486 - val_loss: 0.2892 - val_accuracy: 0.8742\n","Epoch 9/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.3259 - accuracy: 0.8603 - val_loss: 0.2866 - val_accuracy: 0.8750\n","Epoch 10/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.3105 - accuracy: 0.8663 - val_loss: 0.2693 - val_accuracy: 0.8852\n","Epoch 11/20\n","625/625 [==============================] - 54s 86ms/step - loss: 0.2932 - accuracy: 0.8730 - val_loss: 0.2482 - val_accuracy: 0.8942\n","Epoch 12/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.2853 - accuracy: 0.8780 - val_loss: 0.2295 - val_accuracy: 0.9018\n","Epoch 13/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.2690 - accuracy: 0.8870 - val_loss: 0.3435 - val_accuracy: 0.8502\n","Epoch 14/20\n","625/625 [==============================] - 54s 86ms/step - loss: 0.2626 - accuracy: 0.8887 - val_loss: 0.2053 - val_accuracy: 0.9142\n","Epoch 15/20\n","625/625 [==============================] - 53s 85ms/step - loss: 0.2550 - accuracy: 0.8914 - val_loss: 0.2195 - val_accuracy: 0.9074\n","Epoch 16/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.2560 - accuracy: 0.8906 - val_loss: 0.2292 - val_accuracy: 0.9082\n","Epoch 17/20\n","625/625 [==============================] - 54s 86ms/step - loss: 0.2378 - accuracy: 0.9014 - val_loss: 0.2027 - val_accuracy: 0.9192\n","Epoch 18/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.2393 - accuracy: 0.9006 - val_loss: 0.2093 - val_accuracy: 0.9126\n","Epoch 19/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.2376 - accuracy: 0.8997 - val_loss: 0.2052 - val_accuracy: 0.9160\n","Epoch 20/20\n","625/625 [==============================] - 53s 84ms/step - loss: 0.2274 - accuracy: 0.9056 - val_loss: 0.2116 - val_accuracy: 0.9154\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f6df3baed90>"]},"metadata":{},"execution_count":23}],"source":["epochs = 20\n","\n","model6.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n","\n","model6.fit(train_ds, epochs=epochs, validation_data=val_ds)"]},{"cell_type":"markdown","source":["(10, 7pts) Rather than building a model from nearly scratch, a different approach (\"transfer learning\") is to take an existing model that did something similar, including its trained weights, and retrain only a few layers at the very end of the model, keeping the rest of the model \"frozen.\"  This allows complex features learned from a lot of data to be used on smaller problems with less data, and it saves on training time as well.\n","\n","Go to https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/ and use the define_model() function from the section \"Explore Transfer Learning\" as the model this time.  You should keep the \"adam\" optimizer we've been using all along.  You should only need to train to 7 epochs."],"metadata":{"id":"sA1Ez4cRH3O2"},"id":"sA1Ez4cRH3O2"},{"cell_type":"code","execution_count":9,"id":"5d461f2a","metadata":{"id":"5d461f2a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667483951913,"user_tz":240,"elapsed":259038,"user":{"displayName":"Max Karambelas","userId":"10783594860732810487"}},"outputId":"c7c1f238-aa71-43e7-fa6f-b1cd9d940771"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/7\n","625/625 [==============================] - 44s 56ms/step - loss: 166.2894 - accuracy: 0.6026 - val_loss: 18.4338 - val_accuracy: 0.6622\n","Epoch 2/7\n","625/625 [==============================] - 37s 59ms/step - loss: 10.1474 - accuracy: 0.7214 - val_loss: 7.8641 - val_accuracy: 0.6882\n","Epoch 3/7\n","625/625 [==============================] - 35s 55ms/step - loss: 2.6665 - accuracy: 0.8130 - val_loss: 5.6884 - val_accuracy: 0.6634\n","Epoch 4/7\n","625/625 [==============================] - 35s 55ms/step - loss: 1.1114 - accuracy: 0.8681 - val_loss: 5.2528 - val_accuracy: 0.6662\n","Epoch 5/7\n","625/625 [==============================] - 38s 61ms/step - loss: 0.9034 - accuracy: 0.8874 - val_loss: 5.6952 - val_accuracy: 0.6568\n","Epoch 6/7\n","625/625 [==============================] - 35s 55ms/step - loss: 0.6863 - accuracy: 0.9023 - val_loss: 5.5766 - val_accuracy: 0.6546\n","Epoch 7/7\n","625/625 [==============================] - 35s 56ms/step - loss: 0.6589 - accuracy: 0.9090 - val_loss: 5.5529 - val_accuracy: 0.6542\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f550a0f1790>"]},"metadata":{},"execution_count":9}],"source":["# TODO adapt define_model() from https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats/\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Dense\n","from keras.layers import Flatten\n","from keras.optimizers import SGD\n","\n","def define_model():\n","  model7 = Sequential()\n","  model7.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(180, 180, 3)))\n","  model7.add(MaxPooling2D((2, 2)))\n","  model7.add(Flatten())\n","  model7.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n","  model7.add(Dense(1, activation='sigmoid'))\n","\t# compile model\n","  opt = SGD(lr=0.001, momentum=0.9)\n","  model7.compile(optimizer=\"adam\", loss='binary_crossentropy', metrics=['accuracy'])\n","  return model7\n","    # TODO\n","\n","model7 = define_model()\n","model7.fit(train_ds, epochs=7,validation_data=val_ds)"]},{"cell_type":"markdown","source":["(11, 9pts) Last, answer these questions.\n","\n","(a, 3pts) Increasing the complexity of the network, by adding more layers for example, increases the chance that the network will overfit.  Why?"],"metadata":{"id":"efHZL1P6yyA5"},"id":"efHZL1P6yyA5"},{"cell_type":"markdown","source":["**This is because when a network becomes more complex it captures noise in the training data instead of the underlying signal. This means that the network does not generalize well which leads to overfitting.**"],"metadata":{"id":"GssQEsPyh4Nf"},"id":"GssQEsPyh4Nf"},{"cell_type":"markdown","source":["(b, 3pts) In your own words, how does dropout reduce overfitting?"],"metadata":{"id":"laXJ8j3qhlDa"},"id":"laXJ8j3qhlDa"},{"cell_type":"markdown","source":["**Dropout means that neurons cannot rely on one input since it could be dropped out at random, this reduces bias be reducing how much they can rely on one input. Which reduces overfitting since bias is a major cause of overfitting.**"],"metadata":{"id":"SIUjgKExzl2q"},"id":"SIUjgKExzl2q"},{"cell_type":"markdown","source":["(c, 3pts) Transfer learning is effective, and it's more effective the more similar the original task was to the present one.  Skim the Wikipedia page on ImageNet, the dataset used to train VGG-16 (and maybe follow up by searching this subset of ImageNet's classes: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Is there reason to think transfer learning with VGG-16 might be particularly effective for classifying cats and dogs?"],"metadata":{"id":"zvUg7PaZ0PZY"},"id":"zvUg7PaZ0PZY"},{"cell_type":"markdown","source":["**Yes, since VCG-16 is particularly easy to use with transfer learning it would be effective at classifying cats and dogs. Also, since they are both image classification and detection algorithms.**"],"metadata":{"id":"Qu5ylggG3GaV"},"id":"Qu5ylggG3GaV"},{"cell_type":"markdown","source":["**When you're done, use \"File->Download .ipynb\" and upload your .ipynb file to Blackboard, along with a PDF version (File->Print->Save as PDF) of your assignment.**"],"metadata":{"id":"oH6oywqOc04-"},"id":"oH6oywqOc04-"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"provenance":[],"collapsed_sections":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}